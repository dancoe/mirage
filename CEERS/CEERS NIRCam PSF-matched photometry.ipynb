{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIRCam PSF-matched multiband photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "import astropy  # version 4.2 is required to write magnitudes to ecsv file\n",
    "from astropy.io import fits\n",
    "import astropy.wcs as wcs\n",
    "from astropy.table import QTable, Table\n",
    "import astropy.units as u\n",
    "from astropy.visualization import make_lupton_rgb, SqrtStretch, LogStretch, hist, simple_norm\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize\n",
    "from astropy.convolution import Gaussian2DKernel\n",
    "from astropy.stats import gaussian_fwhm_to_sigma\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "import photutils\n",
    "from photutils import Background2D, MedianBackground, detect_sources, deblend_sources, SourceCatalog#, source_properties\n",
    "from photutils.utils import calc_total_error\n",
    "print('photutils', photutils.__version__)\n",
    "from packaging import version\n",
    "if version.parse(photutils.__version__) < version.parse(\"1.4.0\"):\n",
    "    print('WARNING photutils not up to date with 1.4; convolution may take forever and will tweak error input command below')\n",
    "\n",
    "from photutils.psf.matching import resize_psf, create_matching_kernel, CosineBellWindow\n",
    "from astropy.convolution import convolve, convolve_fft # , Gaussian2DKernel, Tophat2DKernel\n",
    "\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "#%matplotlib inline   # non-interactive (easier for notebook scrolling)\n",
    "%matplotlib notebook\n",
    "plt.style.use('https://www.stsci.edu/~dcoe/matplotlibrc.txt') # https://matplotlib.org/tutorials/introductory/customizing.html\n",
    "mpl_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show versions of Python and imported libraries\n",
    "try:\n",
    "    import watermark\n",
    "    %load_ext watermark\n",
    "    # %watermark -n -v -m -g -iv\n",
    "    %watermark -iv -v\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NIRCam INTRAMODULEBOX 4 x Subpixel STANDARD 2\n",
    "10 SHALLOW4 \n",
    "F090W, F150W, F200W, F277W, F356W, F444W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_files_list = glob('../images/*_drz.fits')  \n",
    "image_files_list = glob('../images/*_sci.fits.gz')\n",
    "image_files_list = list(np.sort(image_files_list))\n",
    "filters = [image_file.split('_')[-2] for image_file in image_files_list]\n",
    "image_files = {}\n",
    "wavelengths = np.array([int(filt[1:4]) / 100 for filt in filters]) * u.um  # e.g., F115W = 1.15 microns\n",
    "for i, filt in enumerate(filters):\n",
    "    image_files[filt] = image_files_list[i]\n",
    "    print(filt, image_files[filt])\n",
    "\n",
    "field = os.path.basename(image_files[filt]).split('_')[0]\n",
    "field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Error arrays\n",
    "data_error_files = {}\n",
    "for filt in filters:\n",
    "    #data_error_files[filt] = image_files[filt].replace('_drz.fits', '_err.fits')\n",
    "    data_error_files[filt] = image_files[filt].replace('_sci.fits', '_err.fits')\n",
    "    print(filt, data_error_files[filt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load detection image: F200W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = 1        # extension of HDU where data lives\n",
    "idata_error = 2  # extension of HDU where data_error lives\n",
    "\n",
    "detection_filter = filt = 'F200W'.lower()\n",
    "infile = image_files[filt]\n",
    "hdu = fits.open(infile)\n",
    "data = hdu[idata].data\n",
    "imwcs = wcs.WCS(hdu[idata].header, hdu)\n",
    "data_error = fits.open(data_error_files[filt])[idata_error].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report image size and field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny, nx = data.shape\n",
    "# image_pixel_scale = np.abs(hdu[0].header['CD1_1']) * 3600\n",
    "image_pixel_scale = wcs.utils.proj_plane_pixel_scales(imwcs)[0] \n",
    "image_pixel_scale *= imwcs.wcs.cunit[0].to('arcsec')\n",
    "outline = '%d x %d pixels' % (ny, nx)\n",
    "outline += ' = %g\" x %g\"' % (ny * image_pixel_scale, nx * image_pixel_scale)\n",
    "outline += ' (%.2f\" / pixel)' % image_pixel_scale\n",
    "print(outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9.5, 8))\n",
    "\n",
    "ax = fig.add_subplot(2, 1, 1, projection=imwcs)  # , sharex=True, sharey=True\n",
    "norm = ImageNormalize(stretch=SqrtStretch(), vmin=0, vmax=0.5)\n",
    "plt.imshow(data, origin='lower', norm=norm, interpolation='none', cmap='Greys')\n",
    "plt.xlabel('Right Ascension')\n",
    "plt.ylabel('Declination')\n",
    "\n",
    "ax = fig.add_subplot(2, 1, 2, projection=imwcs)\n",
    "norm = ImageNormalize(stretch=SqrtStretch(), vmin=0, vmax=0.5)\n",
    "plt.imshow(data_error, origin='lower', norm=norm, interpolation='none', cmap='Greys')\n",
    "plt.xlabel('Right Ascension')\n",
    "plt.ylabel('Declination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert measured fluxes (data units) to magnitudes\n",
    "\n",
    "https://docs.astropy.org/en/stable/units/\n",
    "\n",
    "https://docs.astropy.org/en/stable/units/equivalencies.html#photometric-zero-point-equivalency\n",
    "\n",
    "https://docs.astropy.org/en/stable/units/logarithmic_units.html#logarithmic-units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JWST data units: MJy / sr  \n",
    "Mega Janskys (MJy); mag_AB = 8.9 - 2.5 log10(flux_Jy) = 31.4 - 2.5 log10(flux_nJy)  \n",
    "steradian (sr) is a unit of solid angle: sphere = 4 pi sr = 4 pi (180 deg / pi)^2 = 41253 deg^2 = 2.2e16 arcsec^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not detected: mag =  99; magerr = 1-sigma upper limit assuming zero flux\n",
    "# not observed: mag = -99; magerr = 0\n",
    "def fluxes2mags(flux, fluxerr):\n",
    "    nondet = flux < 0  # Non-detection if flux is negative\n",
    "    unobs = (fluxerr <= 0) + (fluxerr == np.inf)  # Unobserved if flux uncertainty is negative or infinity\n",
    "\n",
    "    mag = flux.to(u.ABmag)\n",
    "    magupperlimit = fluxerr.to(u.ABmag) # 1-sigma upper limit if flux=0\n",
    "\n",
    "    mag = np.where(nondet, 99 * u.ABmag, mag)\n",
    "    mag = np.where(unobs, -99 * u.ABmag, mag)\n",
    "\n",
    "    magerr = 2.5 * np.log10(1 + fluxerr/flux) \n",
    "    magerr = magerr.value * u.ABmag\n",
    "\n",
    "    magerr = np.where(nondet, magupperlimit, magerr)\n",
    "    magerr = np.where(unobs, 0 * u.ABmag, magerr)\n",
    "    \n",
    "    return mag, magerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note magnitude uncertainties for detections should probably be u.mag instead of u.ABmag  \n",
    "but magnitude uncertainties for non-detections quote u.ABmag upper limits  \n",
    "They need to be the same, so we go with u.ABmag  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Sources and Deblend using astropy.photutils (similar to SourceExtractor)\n",
    "https://photutils.readthedocs.io/en/latest/segmentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all detection and measurement parameters here so that we do measurements consistently for every image\n",
    "\n",
    "JWST_flux_units = u.MJy / u.sr\n",
    "\n",
    "# https://photutils.readthedocs.io/en/stable/api/photutils.segmentation.SourceCatalog.html#photutils.segmentation.SourceCatalog.area\n",
    "output_properties = 'label xcentroid ycentroid sky_centroid area semimajor_sigma semiminor_sigma'.split()\n",
    "output_properties += 'fwhm ellipticity orientation gini'.split()\n",
    "output_properties += 'kron_radius local_background segment_flux segment_fluxerr kron_flux kron_fluxerr'.split()\n",
    "# columns += 'source_sum source_sum_err kron_flux kron_fluxerr kron_radius local_background'.split()\n",
    "\n",
    "class Photutils_Catalog:\n",
    "    def __init__(self, filt, image_file=None, verbose=True, mask_edge_thickness=10):\n",
    "        self.image_file = image_file or image_files[filt]\n",
    "        self.hdu = fits.open(self.image_file)\n",
    "        self.imwcs = wcs.WCS(self.hdu[idata].header, self.hdu)\n",
    "        self.data       = self.hdu[idata].data  # 'SCI' i2d extension 1\n",
    "        self.convolved_data = None  # initialize; to be populated later; added in photutils 1.4\n",
    "\n",
    "        # total error array (i.e., the background-only error plus Poisson noise due to individual sources)\n",
    "        # https://photutils.readthedocs.io/en/stable/segmentation.html#photometric-errors        \n",
    "        #self.data_error = self.hdu[2].data  # 'ERR' i2d extension 2\n",
    "        self.data_error_file = data_error_files[filt]\n",
    "        self.data_error = fits.open(self.data_error_file)[idata_error].data\n",
    "        self.data_mask = np.isnan(self.data_error) # | (model.wht == 0)\n",
    "        # Remove edge detections: Grow the mask by 10 pixels\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.binary_dilation.html\n",
    "        self.data_mask = ndimage.binary_dilation(self.data_mask, iterations=mask_edge_thickness)\n",
    "\n",
    "        # image_pixel_scale = np.abs(hdu[0].header['CD1_1']) * 3600\n",
    "        self.pixel_scale = wcs.utils.proj_plane_pixel_scales(self.imwcs)[0] \n",
    "        self.pixel_scale *= imwcs.wcs.cunit[0].to('arcsec')\n",
    "        self.flux_units = JWST_flux_units * (self.pixel_scale * u.arcsec)**2\n",
    "        #self.zeropoint = self.hdu[0].header['ABMAG'] * u.ABmag \n",
    "        #self.zeropoint = 31.4 * u.ABmag # no zeropoint (ABMAG) in the header, add manually, 31.4 for nJy\n",
    "        if verbose:\n",
    "            print(self.image_file)\n",
    "            ny, nx = self.data.shape\n",
    "            outline = '%d x %d pixels' % (ny, nx)\n",
    "            outline += ' = %g\" x %g\"' % (ny * self.pixel_scale, nx * self.pixel_scale)\n",
    "            outline += ' (%.2f\" / pixel)' % self.pixel_scale\n",
    "            print(outline)\n",
    "            #print(filt, '  zeropoint =', self.zeropoint)\n",
    "            #print(self.weight_file)\n",
    "\n",
    "    def measure_background_map(self, bkg_size=50, filter_size=3, verbose=True):\n",
    "        # Calculate sigma-clipped background in cells of 50x50 pixels, then median filter over 3x3 cells\n",
    "        # For best results, the image should span an integer number of cells in both dimensions (e.g., 1000=20x50 pixels)\n",
    "        # https://photutils.readthedocs.io/en/stable/background.html\n",
    "        self.background_map = Background2D(self.data, bkg_size, filter_size=filter_size)\n",
    "\n",
    "    def smooth_data(self, smooth_fwhm=2, kernel_size=5):\n",
    "        # convolve data with Gaussian \n",
    "        # convolved_data used for source detection and to calculate source centroids and morphological properties\n",
    "        smooth_sigma = smooth_fwhm * gaussian_fwhm_to_sigma\n",
    "        self.smooth_kernel = Gaussian2DKernel(smooth_sigma, x_size=kernel_size, y_size=kernel_size)\n",
    "        self.smooth_kernel.normalize()\n",
    "        self.convolved_data = convolve(self.data, self.smooth_kernel)\n",
    "        \n",
    "    def run_detect_sources(self, nsigma, npixels, smooth_fwhm=2, kernel_size=5, \n",
    "                           deblend_levels=32, deblend_contrast=0.001, verbose=True):\n",
    "\n",
    "        # Set detection threshold map as nsigma times RMS above background pedestal\n",
    "        detection_threshold = (nsigma * self.background_map.background_rms) + self.background_map.background\n",
    "\n",
    "        # Before detection, convolve data with Gaussian\n",
    "        self.smooth_data(smooth_fwhm, kernel_size)\n",
    "\n",
    "        # Detect sources with npixels connected pixels at/above threshold in data smoothed by kernel\n",
    "        # https://photutils.readthedocs.io/en/stable/segmentation.html\n",
    "        self.segm_detect = detect_sources(self.data, detection_threshold, npixels=npixels, kernel=self.smooth_kernel)\n",
    "\n",
    "        # Deblend: separate connected/overlapping sources\n",
    "        # https://photutils.readthedocs.io/en/stable/segmentation.html#source-deblending\n",
    "        self.segm_deblend = deblend_sources(self.data, self.segm_detect, npixels=npixels, kernel=self.smooth_kernel,\n",
    "                                            nlevels=deblend_levels, contrast=deblend_contrast)\n",
    "        if verbose:\n",
    "            output = 'Cataloged %d objects' % self.segm_deblend.nlabels\n",
    "            output += ', deblended from %d detections' % self.segm_detect.nlabels\n",
    "            median_threshold = (nsigma * self.background_map.background_rms_median) \\\n",
    "                + self.background_map.background_median\n",
    "            output += ' with %d pixels above %g-sigma threshold' % (npixels, nsigma)\n",
    "            # Background outputs equivalent to those reported by SourceExtractor\n",
    "            output += '\\n'\n",
    "            output += 'Background median %g' % self.background_map.background_median\n",
    "            output += ', RMS %g' % self.background_map.background_rms_median\n",
    "            output += ', threshold median %g' % median_threshold\n",
    "            print(output)\n",
    "\n",
    "    def measure_source_properties(self, local_background_width=24, properties=output_properties):\n",
    "        if version.parse(photutils.__version__) >= version.parse(\"1.4.0\"):\n",
    "            self.catalog = SourceCatalog(self.data-self.background_map.background, self.segm_deblend,\n",
    "                                         convolved_data=self.convolved_data,  # photutils 1.4\n",
    "                                         error=self.data_error, mask=self.data_mask,\n",
    "                                         background=self.background_map.background, wcs=self.imwcs,\n",
    "                                         localbkg_width=local_background_width)\n",
    "        else:  # use filter_kernel instead of convolved_data\n",
    "            self.catalog = SourceCatalog(self.data-self.background_map.background, self.segm_deblend, \n",
    "                                         kernel=self.smooth_kernel,  # photutils < 1.4 \n",
    "                                         error=self.data_error, mask=self.data_mask,\n",
    "                                         background=self.background_map.background, wcs=self.imwcs,\n",
    "                                         localbkg_width=local_background_width)\n",
    "\n",
    "\n",
    "        self.catalog_table = self.catalog.to_table(columns=properties)  # properties: quantities to keep\n",
    "        \n",
    "        # Convert fluxes to nJy units and to AB magnitudes\n",
    "        for aperture in ['segment', 'kron']:\n",
    "            flux    = self.catalog_table[aperture+'_flux']    * self.flux_units.to(u.nJy)\n",
    "            fluxerr = self.catalog_table[aperture+'_fluxerr'] * self.flux_units.to(u.nJy)\n",
    "            mag, magerr = fluxes2mags(flux, fluxerr)\n",
    "            \n",
    "            self.catalog_table[aperture+'_flux']    = flux\n",
    "            self.catalog_table[aperture+'_fluxerr'] = fluxerr\n",
    "            self.catalog_table[aperture+'_mag']     = mag\n",
    "            self.catalog_table[aperture+'_magerr']  = magerr            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_catalog = Photutils_Catalog(detection_filter)\n",
    "detection_catalog.measure_background_map()\n",
    "detection_catalog.run_detect_sources(nsigma=3, npixels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_catalog = Photutils_Catalog(detection_filter)\n",
    "detection_catalog.measure_background_map()\n",
    "detection_catalog.run_detect_sources(nsigma=3, npixels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove masked segments (labels)\n",
    "# https://photutils.readthedocs.io/en/latest/api/photutils.segmentation.SegmentationImage.html#photutils.segmentation.SegmentationImage.remove_masked_labels\n",
    "detection_catalog.segm_deblend.remove_masked_labels(detection_catalog.data_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save segmentation map of detected objects\n",
    "segm_hdu = fits.PrimaryHDU(detection_catalog.segm_deblend.data.astype(np.uint32), header=imwcs.to_header())\n",
    "outroot = field + '_' + detection_filter\n",
    "segm_hdu.writeto(outroot+'_detections_segm.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_catalog.measure_source_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection_catalog.catalog_table  # show contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak catalog\n",
    "source_table = detection_catalog.catalog_table\n",
    "source_table.rename_column('label', 'id')\n",
    "source_table.rename_column('semimajor_sigma', 'a')\n",
    "source_table.rename_column('semiminor_sigma', 'b')\n",
    "source_table.rename_column('xcentroid', 'x')\n",
    "source_table.rename_column('ycentroid', 'y')\n",
    "\n",
    "# Replace sky_centroid with ra, dec\n",
    "source_table['ra']  = source_table['sky_centroid'].ra.degree  * u.degree\n",
    "source_table['dec'] = source_table['sky_centroid'].dec.degree * u.degree\n",
    "\n",
    "columns = list(source_table.columns)\n",
    "columns = columns[:3] + ['ra', 'dec'] + columns[4:-2]\n",
    "\n",
    "source_table = source_table[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If interested, view / save output, but photometry in other filters will be added soon!\n",
    "if 0:\n",
    "    source_table.write(outroot+'_detections.ecsv', overwrite=True)\n",
    "    source_table.write(outroot+'_detections.cat', format='ascii.fixed_width_two_line', delimiter=' ', overwrite=True)\n",
    "    #source_table  # show contents\n",
    "    outroot+'_detections.cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show detections (segmentation map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9.5,5), sharex=True, sharey=True)\n",
    "\n",
    "norm = simple_norm(detection_catalog.data, 'sqrt', min_percent=0.1, max_percent=99.9)\n",
    "ax[0].imshow(detection_catalog.data, origin='lower', interpolation='none', norm=norm, cmap='Greys_r' )\n",
    "#ax[0].plot(source_table['x'], source_table['y'], 'mo', mfc='None')\n",
    "\n",
    "cmap = detection_catalog.segm_deblend.make_cmap(seed=123)\n",
    "ax[1].imshow(detection_catalog.segm_deblend, origin='lower', interpolation='none', cmap=cmap)\n",
    "\n",
    "for ix in range(2):\n",
    "    ax[ix].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiband photometry using isophotal apertures defined in detection image\n",
    "(Similar to running SourceExtractor in double-image mode)  \n",
    "(No PSF corrections just yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filt in filters:\n",
    "    filter_catalog = Photutils_Catalog(filt)\n",
    "    filter_catalog.measure_background_map()\n",
    "    filter_catalog.segm_deblend = detection_catalog.segm_deblend\n",
    "    filter_catalog.measure_source_properties()\n",
    "    \n",
    "    source_table[filt+'_flux']    = filter_catalog.catalog_table['segment_flux']\n",
    "    source_table[filt+'_fluxerr'] = filter_catalog.catalog_table['segment_fluxerr']\n",
    "\n",
    "    source_table[filt+'_mag']     = filter_catalog.catalog_table['segment_flux']\n",
    "    source_table[filt+'_magerr']  = filter_catalog.catalog_table['segment_magerr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperture corrections: isophotal to total (Kron aperture) fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flux_table = deepcopy(source_table) # copy to new table, which will include total magnitude corrections\n",
    "\n",
    "reference_flux_auto = total_flux_table['kron_flux']    # Kron total flux estimate\n",
    "reference_flux_iso  = total_flux_table['segment_flux'] # flux in isophotal aperture defined by detection segment\n",
    "kron_flux_corrections = reference_flux_auto / reference_flux_iso\n",
    "total_flux_table['total_flux_cor'] = kron_flux_corrections\n",
    "\n",
    "for filt in filters:\n",
    "    total_flux_table[filt+'_flux']    *= kron_flux_corrections\n",
    "    total_flux_table[filt+'_fluxerr'] *= kron_flux_corrections\n",
    "    #total_flux_table[filt+'_mag'] = total_flux_table[filt+'_flux'].to(u.ABmag)  # doesn't handle non-detections\n",
    "    total_flux_table[filt+'_mag'] = fluxes2mags(total_flux_table[filt+'_flux'], total_flux_table[filt+'_fluxerr'])[0]\n",
    "    # magnitude uncertainty magerr stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat output catalog for readability (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isophotal_table = deepcopy(total_flux_table) # copy to new table, which will include PSF-corrections\n",
    "\n",
    "old_columns = list(isophotal_table.columns)\n",
    "\n",
    "# Reorder columns\n",
    "i = old_columns.index('segment_flux')\n",
    "j = old_columns.index(filters[0]+'_flux')\n",
    "columns = old_columns[:i] # detection catalog (except source_sum & kron_flux)\n",
    "columns += old_columns[-1:]  # total_flux_cor\n",
    "columns += old_columns[j:-1] # photometry in all filters\n",
    "        \n",
    "isophotal_table = isophotal_table[columns]\n",
    "\n",
    "for column in columns:\n",
    "    isophotal_table[column].info.format = '.4f'\n",
    "\n",
    "isophotal_table['ra'].info.format  = '11.7f'\n",
    "isophotal_table['dec'].info.format = ' 11.7f'\n",
    "\n",
    "isophotal_table['id'].info.format = 'd'\n",
    "isophotal_table['area'].info.format = '.0f'  # 'd' raises error : incompatible with units (pix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isophotal Photometry without PSF corrections complete\n",
    "\n",
    "We recommend proceeding with PSF corrections to photometry in the Long Wavelength Channel. But if you are interested, you may save the catalog now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    isophotal_table.write(field+'_isophotal_photometry.ecsv', overwrite=True)\n",
    "    isophotal_table.write(field+'_isophotal_photometry.cat', format='ascii.fixed_width_two_line', delimiter=' ', overwrite=True)\n",
    "    #isophotal_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSF magnitude corrections\n",
    "\n",
    "Color corrections are perfomed as described here:\n",
    "https://www.stsci.edu/~dcoe/ColorPro/color\n",
    "\n",
    "Note this is different from one common approach, which is to degrade every image to the broadest PSF.\n",
    "\n",
    "Photometry is only corrected in Long Wavelength images > 2.4 microns. The F200W detection image is convolved (blurred) to the PSF of each Long Wavelength image. Then we correct colors based on the magnitudes lost in that aperture:\n",
    "\n",
    "* PSF_magnitude_corrections = detection_image_magnitudes - blurred_detection_image_magnitudes\n",
    "\n",
    "In practice, we actually correct the fluxes:\n",
    "\n",
    "* PSF_flux_corrections = detection_image_fluxes / blurred_detection_image_fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PSFs\n",
    "\n",
    "NIRCam PSF files available via:  \n",
    "https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/nircam_photometry/NIRCam_PSFs/\n",
    "\n",
    "...were extracted from tar files available on JDox:  \n",
    "https://jwst-docs.stsci.edu/near-infrared-camera/nircam-predicted-performance/nircam-point-spread-functions#NIRCamPointSpreadFunctions-SimulatedNIRCamPSFs  \n",
    "PSFs_SW_filters (short wavelength channel): https://stsci.box.com/s/s2lepxr9086gq4sogr3kwftp54n1c5vl  \n",
    "PSFs_LW_filters (long wavelength channel): https://stsci.box.com/s/gzl7blxb1k3p4n66gs7jvt7xorfrotyb  \n",
    "\n",
    "Each FITS file contains:  \n",
    "– hdu[0]: a 4x oversampled PSF  \n",
    "– hdu[1]: PSF at detector pixel scale (0.031\" and 0.063\" in the short and long wavelength channels, respectively)  \n",
    "This notebook will use the latter: PSF at detector pixel scale. We find no advantage to the former for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSF_inputs = {}\n",
    "PSF_images = {}\n",
    "\n",
    "detector_pixel_scales = {'SW': 0.031, 'LW': 0.063}\n",
    "\n",
    "#input_file_url = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/nircam_photometry/'\n",
    "#PSF_directory = os.path.join(input_file_url, 'NIRCam_PSFs')\n",
    "# missing F480M\n",
    "\n",
    "PSF_directory1 = os.path.join(home, 'NIRCam/photometry/PSFmatched/')\n",
    "ny_resize = 0  # make them all the same size: the size of the first one measured\n",
    "# Need cropped to same size to calculate convolution kernels\n",
    "\n",
    "for i, filt in enumerate(filters):\n",
    "    lam = wavelengths[i]\n",
    "    if lam < 2.4 * u.um:\n",
    "        channel = 'SW'  # Short Wavelength Channel < 2.4 microns\n",
    "    else:\n",
    "        channel = 'LW'  # Long Wavelength Channel > 2.4 microns\n",
    "    \n",
    "    PSF_directory = os.path.join(PSF_directory1, 'NIRCam_PSFs_'+channel)\n",
    "\n",
    "    # Load PSF\n",
    "    PSF_file = 'PSF_%scen_G5V_fov299px_ISIM41.fits' % filt\n",
    "    # PSF_file = os.path.join('NIRCam_PSFs_' + channel, PSF_file)\n",
    "    PSF_file = os.path.join(PSF_directory, PSF_file)\n",
    "\n",
    "    print(PSF_file)\n",
    "    PSF_hdu = fits.open(PSF_file)\n",
    "    PSF_inputs[filt] = data = PSF_hdu[1].data  # extension [1] is at pixel scale (not oversampled)\n",
    "    ny, nx = data.shape\n",
    "    \n",
    "    # Resize from detector pixel scale to image pixel scale (here 0.02\" / pix)\n",
    "    detector_pixel_scale = detector_pixel_scales[channel]\n",
    "    if not ny_resize:  # calculate once, then use for all PSFs\n",
    "        ny_resize = ny * detector_pixel_scale / image_pixel_scale  # Assume square PSF (ny = nx)\n",
    "        ny_resize = np.round(ny_resize)\n",
    "        ny_resize = int((ny_resize // 2) * 2 + 1)  # Make it an odd number of pixels to ensure PSF is centered\n",
    "        \n",
    "    PSF_pixel_scale = ny_resize / ny * image_pixel_scale    \n",
    "    PSF_image = resize_psf(PSF_inputs[filt], PSF_pixel_scale, image_pixel_scale)  # Resize PSF here\n",
    "    r = (ny_resize - ny) // 2\n",
    "    if r > 0:  # new size bigger than original\n",
    "        PSF_image = PSF_image[r:-r, r:-r]  # Trim to same size as input PSFs\n",
    "    PSF_images[filt] = PSF_image\n",
    "    # Note PSF is no longer normalized but will be later in convolution step\n",
    "    print(filt, ny, ny_resize, PSF_image.shape, PSF_images[filt].shape, PSF_pixel_scale, np.sum(PSF_images[filt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show PSFs (optional)\n",
    "\n",
    "fig, ax = plt.subplots(1, len(filters), figsize=(9.5, 1.5), sharex=True, sharey=True)\n",
    "\n",
    "r = 15  # PSF will be shown out to radius r (pixels)\n",
    "for i, filt in enumerate(filters):\n",
    "    data = PSF_images[filt]\n",
    "    ny, nx = data.shape\n",
    "    yc = ny // 2\n",
    "    xc = nx // 2\n",
    "    stamp = data[yc-r:yc+r+1, xc-r:xc+r+1]\n",
    "    norm = ImageNormalize(stretch=LogStretch())  # scale each filter individually\n",
    "    ax[i].imshow(stamp, cmap='Greys_r', norm=norm, origin='lower')\n",
    "    ax[i].set_title(filt.upper())\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSF Matching\n",
    "\n",
    "https://photutils.readthedocs.io/en/stable/psf_matching.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine PSF convolution kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSF_kernels = {}\n",
    "reference_filter = 'f200w'\n",
    "reference_PSF = PSF_images[reference_filter]\n",
    "i_reference = filters.index(reference_filter)\n",
    "window = CosineBellWindow(alpha=0.35)\n",
    "for filt in filters[i_reference+1:]:  # only for longer wavelength filters\n",
    "    PSF_kernels[filt] = create_matching_kernel(reference_PSF, PSF_images[filt], window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_image_hdu = fits.open(image_files[reference_filter])\n",
    "reference_image_data = reference_image_hdu[idata].data[:]\n",
    "\n",
    "for output_filter in filters[i_reference+1:]:\n",
    "    output_image = '%s_convolved_%s_to_%s.fits' % (field, reference_filter, output_filter)\n",
    "    if os.path.exists(output_image):\n",
    "        print(output_image, 'EXISTS')\n",
    "    else:\n",
    "        print(output_filter + '...')\n",
    "        PSF_kernel = PSF_kernels[output_filter][yc-r:yc+r+1, xc-r:xc+r+1]\n",
    "        # convolve_fft may be faster than convolve for larger images / kernels\n",
    "        convolved_image = convolve_fft(reference_image_data, PSF_kernel, normalize_kernel=True)\n",
    "        reference_image_hdu[idata].data = convolved_image\n",
    "        print('SAVING %s' % output_image)\n",
    "        reference_image_hdu = reference_image_hdu[:3]  # only save extensions 0 (header) and 1 ('SCI') and 2 ('ERR')\n",
    "        # 'ERR' isn't blurred, but keep it to get this file through the analysis\n",
    "        reference_image_hdu.writeto(output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiband photometry in convolved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure and save the fluxes in each blurry image\n",
    "blurry_catalog = QTable()\n",
    "\n",
    "for blurry_filter in filters[i_reference+1:]:\n",
    "    image_file = '%s_convolved_%s_to_%s.fits' % (field, reference_filter, blurry_filter)\n",
    "    filter_catalog = Photutils_Catalog(blurry_filter, image_file=image_file)\n",
    "    filter_catalog.measure_background_map()\n",
    "\n",
    "    # Measure photometry in this filter for objects detected in detected image\n",
    "    # segmentation map will define isophotal apertures\n",
    "    filter_catalog.segm_deblend = detection_catalog.segm_deblend\n",
    "    filter_catalog.measure_source_properties()\n",
    "\n",
    "    # Convert measured fluxes to fluxes in nJy\n",
    "    filter_table = filter_catalog.catalog.to_table()\n",
    "    blurry_catalog[blurry_filter+'_flux'] = filter_catalog.catalog_table['segment_flux']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSF magnitude corrections\n",
    "\n",
    "https://www.stsci.edu/~dcoe/ColorPro/color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSF_corrected_table = deepcopy(isophotal_table)\n",
    "\n",
    "reference_fluxes = PSF_corrected_table[reference_filter+'_flux']  # det_flux_auto\n",
    "\n",
    "for filt in filters[i_reference+1:]:\n",
    "    # Convert isophotal fluxes to total fluxes\n",
    "    blurry_total_fluxes = blurry_catalog[filt+'_flux'] * kron_flux_corrections\n",
    "    PSF_flux_corrections = reference_fluxes / blurry_total_fluxes\n",
    "    PSF_corrected_table[filt+'_flux']    *= PSF_flux_corrections\n",
    "    PSF_corrected_table[filt+'_fluxerr'] *= PSF_flux_corrections\n",
    "    # PSF_corrected_table[filt+'_mag'] = PSF_corrected_fluxes.to(u.ABmag)  # doesn't handle non-detections\n",
    "    PSF_corrected_table[filt+'_mag'], PSF_corrected_table[filt+'_magerr'] = \\\n",
    "        fluxes2mags(PSF_corrected_table[filt+'_flux'], PSF_corrected_table[filt+'_fluxerr'])\n",
    "    PSF_corrected_table[filt+'_PSF_flux_cor'] = PSF_flux_corrections    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flux in uJy (should have done that above)\n",
    "for filt in filters:\n",
    "    PSF_corrected_table[filt+'_flux']    = PSF_corrected_table[filt+'_flux'].to(u.uJy)\n",
    "    PSF_corrected_table[filt+'_fluxerr'] = PSF_corrected_table[filt+'_fluxerr'].to(u.uJy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSF_corrected_table.write(field+'_photometry.ecsv', overwrite=True)\n",
    "PSF_corrected_table.write(field+'_photometry.cat', format='ascii.fixed_width_two_line', delimiter=' ', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSF_corrected_table.write(field+'_photometry.csv', overwrite=True)  # for EAZY with flux in uJy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field+'_photometry.cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSF_corrected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
